In this lecture, Mr. Shah Nawaz talked about Multimodal Representational Learning. Modality means; the way in which something is expressed or perceived, so for something to be "Multimodal" would mean that it has many ways of expressing, or being expressed.
First, Mr. Nawaz went over a few examples of "Task Specific Multimodal Methods" for "F-V Association (face-voice)". They showed that if a model was trained to recognize a particular language, it would successfully map faces to voices that appeared to speak that language, with a low error rate. However, the same model would perform poorly on an "unheard" language, demonstrating the trouble that multilingual systems face when presented with a similar task.
Next, Mr. Nawaz described the differences between a "single-branch" and "multi-branch" network, and how both networks capitalize on the complexity of human features/characteristics. For example, a Multi-branch network would learn the embedding of each modality independently and then join them at the end to form a "joint representation". Meanwhile the Single-branch network learns the embedding of all modalities together, which gets you the same result, however it doesn't rely on the availability of a "complete set of modalities". It is able to form a representation even if some modalities are missing.
Finally, Shah ended with bringing attention to "Gemini", a "family of large language models developed by Google DeepMind" that were designed to handle multiple modalities. They can be used, for example, to create a recipe for Choc-Chip cookies, or to interpret an image with a word or phrase.
Overall, I found this lecture to be quite interesting, however a lot of what was covered flew over my head, as I am not very knowledgeable on the topic, but the visual-aids helped a lot in clearing the air. I'd recommend this to other 1st Semester students as it covers a very important topic in the future of machines interpreting the world around us.
